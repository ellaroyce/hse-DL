{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction via Deep NLP methods\n",
    "A recent Kaggle competition (merely 5 years) propose you to find out who is the most well-paid professional\n",
    "\n",
    "We are gonna solve regression task.\n",
    "\n",
    "Competition is available here: https://www.kaggle.com/c/job-salary-prediction\n",
    "\n",
    "![Hobby](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "In this notebook you will learn\n",
    " - Data preprocessing for NLP or the most annoying part of data scientist's job\n",
    " - Convolutional Neural Networks for texts\n",
    " - Constructing you NN with scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset is consists of job data. Most of it is an unstructured text. You should predict annual salary\n",
    "\n",
    "Download here: https://yadi.sk/d/vVEOWPFY3NruT7 or from competition\n",
    "\n",
    "You need to download Train_rev1.zip and unpack it\n",
    "## Main fields\n",
    "\n",
    "Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n",
    "\n",
    "FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n",
    "\n",
    "LocationRaw - The freetext location as provided by the job advertiser.\n",
    "\n",
    "LocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!\n",
    "\n",
    "ContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "ContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "Company - the name of the employer as supplied to us by the job advertiser.\n",
    "\n",
    "Category - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field.\n",
    "\n",
    "## Desiered field\n",
    "SalaryRaw - the freetext salary field we received in the job advert from the advertiser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame.from_csv('Train_rev1.csv', index_col=None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.ContractType = X.ContractType.astype(str)\n",
    "X.ContractTime = X.ContractTime.astype(str)\n",
    "X.Company = X.Company.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = X['SalaryNormalized'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del X['Id'],  X['SalaryRaw'], X['SourceName'], X['SalaryNormalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f9d1bf7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(Y, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "At this very stage we will distill valuable data out of the dataset\n",
    "\n",
    "First of all - let's remove rare tokens and finalaize our dictionary\n",
    "\n",
    "We count all tokens ever occurred in any of `text_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'FullDescription', 'LocationRaw', 'LocationNormalized', 'ContractType', 'ContractTime', 'Company', 'Category']\n"
     ]
    }
   ],
   "source": [
    "text_columns = list(X.columns)\n",
    "print(text_columns)\n",
    "categorial_colums = ['ContractType', 'ContractTime', 'Company', 'Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code is below. \n",
    "\n",
    "Remember to apply .lower() to all strings before tokenization\n",
    "\n",
    "Consider using tqdm_notebook for not dying  during the iteration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    \"\"\" Returns a list of string tokens\n",
    "    Arguments:\n",
    "    value -- string to tokenize\n",
    "    \"\"\"\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "for i in text_columns:\n",
    "    for s in (X[i].values):\n",
    "        try:\n",
    "            token_counts.update(tokenize(s))\n",
    "        except:\n",
    "            print(i)\n",
    "\n",
    "# <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 2663215)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('and', 2663215)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2663215"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205062"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to actually build token dict. We will use only words that occur more than `min_count` times in dataset\n",
    "\n",
    "Fill two dict mappings id->token and token->id\n",
    "\n",
    "**Minimum id is 2**, because 0 is reserved for padding and 1 is UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c > min_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,34000)\n",
    "assert 'me' in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "id_to_word = dict()\n",
    "word_to_id = dict()\n",
    "\n",
    "token_to_id = {t:i+2 for i,t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {i+2:t for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remember to:\n",
    "     - Transform a string to list of tokens\n",
    "     - Transform each token to it ids (if not in the dict, replace with UNK)\n",
    "     - Pad each line to max_len'''\n",
    "    max_len = 0\n",
    "    token_matrix = []\n",
    "    \n",
    "    for i in range(len(strings)):\n",
    "        t=tokenize(strings[i])\n",
    "        max_len = max(max_len, len(t))\n",
    "        token_matrix.append([])\n",
    "        for k in t:\n",
    "            if k not in token_to_id:\n",
    "                token_matrix[i].append(UNK)\n",
    "            else:\n",
    "                token_matrix[i].append(token_to_id[k])\n",
    "    for i in range(len(token_matrix)):\n",
    "        while(len(token_matrix[i])<max_len):\n",
    "            token_matrix[i].append(0)\n",
    "            \n",
    "    \n",
    "#     <YOUR CODE HERE>\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas\", \"data\"], token_to_id, 1)\n",
    "assert test.shape==(2,2)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you successfully completed all tasks by this moment, you get (2 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True deep learning\n",
    "\n",
    "Now we will define our convolutional neural network.\n",
    "\n",
    "We will think about categorical fields as a sequential - but we won't apply CNN to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialaize some placeholders for data\n",
    "\n",
    "What size it should  be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders = dict()\n",
    "for col in text_columns:\n",
    "    placeholders[col] = tf.placeholder(dtype=tf.int32,\n",
    "                                        shape=[None, None],\n",
    "                                        name=\"word_ids_%s\"%col)\n",
    "    \n",
    "true_y = tf.placeholder(dtype=tf.float32, shape = [None], name=\"true_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are vector representations for tokens. Basically it is just a table where each token (represented by it's id) has a vector representing its sense.\n",
    "\n",
    "It will be learned simultaneously with other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 128\n",
    "word_embeddings_matrix = tf.get_variable(name=\"word_embeddings_matrix\",\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.random_normal(\n",
    "                                             shape=[dict_size, embeddings_size], #<your_code_here>\n",
    "                                             stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep nets\n",
    "Below we are gonna define some network architectures corresponding to each input (a column from a source data)\n",
    "\n",
    "`tf.layers.conv1d()` has three parameters. \n",
    " - inputs is a tensor with [batch_size, time, embedding_dimension]\n",
    " - filters is an int number which means number of channels on the output\n",
    " - kernel_size is an int number which means a size of input data which will be convoluted\n",
    "\n",
    "**Please, specify a padding='same'. This will prevent errors in case of low **\n",
    "`tf.layers.conv1d()` returns a tensor with dimension  [batch_size, time, filters]\n",
    "\n",
    "`tf.layers.max_pooling1d()` is prerry much the same. The only thing you need to specify - pool size and strides.\n",
    "\n",
    "For global pooling you can use `tf.reduce_max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "drop = 0.5\n",
    "sequence_length = max(len(w) for w in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3464.55935\n",
    "\n",
    "def dream_neural_net(word_ids, name):\n",
    "    \"\"\"Function takes an int32 matrix [batch_size, time] and returnes infered [batch_size, dense_vector] \n",
    "    matrix with a conv and pooling layers, finished by a global pooling\"\"\"\n",
    "    with tf.variable_scope(name_or_scope=\"conv_\" + name):\n",
    "        \n",
    "        output = tf.nn.embedding_lookup(params=word_embeddings_matrix,\n",
    "                                             ids=word_ids) #returns [batch_size, time, embedding_dimension]\n",
    "#         <YOUR CODE HERE>\n",
    "    \n",
    "    inputSize = int(output.get_shape()[-1])\n",
    "\n",
    "    output0 = tf.layers.conv1d(inputs=output, \n",
    "                              filters=sequence_length - filter_sizes[0] + 1, \n",
    "                              kernel_size=filter_sizes[0],  \n",
    "                              padding='same',\n",
    "                              activation=tf.nn.elu)\n",
    "    \n",
    "    output0 = tf.reduce_max(output0,axis=1)\n",
    "    \n",
    "    output1 = tf.layers.conv1d(inputs=output, \n",
    "                              filters=sequence_length - filter_sizes[1] + 1, \n",
    "                              kernel_size=filter_sizes[1],  \n",
    "                              padding='same',\n",
    "                              activation=tf.nn.elu)\n",
    "    \n",
    "    output1 = tf.reduce_max(output1,axis=1)\n",
    "    \n",
    "    output2 = tf.layers.conv1d(inputs=output, \n",
    "                              filters=sequence_length - filter_sizes[2] + 1, \n",
    "                              kernel_size=filter_sizes[2],  \n",
    "                              padding='same',\n",
    "                              activation=tf.nn.elu)\n",
    "    \n",
    "    output2 = tf.reduce_max(output2,axis=1)\n",
    "    \n",
    "    merged_tensor = tf.concat([output0, output1, output2],axis=1)\n",
    "    \n",
    "    output = merged_tensor\n",
    "    \n",
    "    dropout = tf.layers.dropout(merged_tensor, rate=drop)\n",
    "    \n",
    "    output = dropout\n",
    "\n",
    "#     output = tf.reduce_mean(output, axis=0)\n",
    "#     output = tf.layers.dense(output, units=128, activation=tf.nn.softmax)\n",
    "    \n",
    "    print(\"num \", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net_categorial(word_ids, name):\n",
    "    \"\"\"Function takes an int32 matrix [batch_size, time] and returnes infered [batch_size, latent_dim] \n",
    "    matrix without a conv and pooling layers, only dense layers\"\"\"\n",
    "    output = tf.nn.embedding_lookup(params=word_embeddings_matrix,\n",
    "                                         ids=word_ids) #returns [batch_size, time, embedding_dimension]\n",
    "\n",
    "#     output = tf.layers.max_pooling1d(inputs=output, \n",
    "#                                      pool_size=sequence_length - filter_sizes[0] + 1, \n",
    "#                                      strides=1, \n",
    "#                                      padding='same')\n",
    "    output = tf.reduce_sum(output,axis=1)\n",
    "    print(\"cat \", output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to combine all architectures. In a dict below you can match input type and an architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nets_types = {'FullDescription': dream_neural_net, \n",
    " 'LocationRaw': dream_neural_net, \n",
    " 'Title': dream_neural_net, \n",
    "              \n",
    " 'LocationNormalized': dream_neural_net_categorial,             \n",
    " 'ContractType': dream_neural_net_categorial, \n",
    " 'ContractTime': dream_neural_net_categorial, \n",
    " 'Company': dream_neural_net_categorial, \n",
    " 'Category':dream_neural_net_categorial\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num  (?, 225)\n",
      "num  (?, 225)\n",
      "num  (?, 225)\n",
      "cat  (?, 128)\n",
      "cat  (?, 128)\n",
      "cat  (?, 128)\n",
      "cat  (?, 128)\n",
      "cat  (?, 128)\n"
     ]
    }
   ],
   "source": [
    "#here we apply specified architecture for each input \n",
    "outputs_to_concat = [nets_types[name](word_ids, name) for name, word_ids in placeholders.items()] \n",
    "dense_repr = tf.concat(outputs_to_concat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1315)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_repr.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(dense_repr.get_shape().as_list())==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_to_number(dense_inputs):\n",
    "    \n",
    "    output = tf.layers.dense(dense_inputs, 50) \n",
    "    output = tf.contrib.layers.fully_connected(inputs=output,\n",
    "                                               num_outputs=1,\n",
    "                                               activation_fn=None,\n",
    "                                               weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                               biases_initializer=tf.zeros_initializer(),\n",
    "                                               scope=\"dense_2\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_output = reduce_to_number(dense_repr)\n",
    "predicted_y = tf.reshape(net_output, (-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_output.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert net_output.get_shape().as_list()==[None, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Remember - we have a regression task, what would be the loss?\n",
    "\n",
    "Also we will estimate a target metric for a competition - **Mean absolute error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean((predicted_y - true_y)**2)\n",
    "\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + 0.01 * \\\n",
    "#                       tf.nn.l2_loss(weights['h1']) + 0.01 * tf.nn.l2_loss(weights['out']))\n",
    "mean_abs_error = tf.reduce_mean(tf.abs(predicted_y - true_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#same, but more detailed:\n",
    "#updates = [[tf.gradients(loss,y_guess)[0], y_guess]]\n",
    "#optimizer = tf.train.MomentumOptimizer(0.01,0.9).apply_gradients(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer   = tf.train.AdamOptimizer(learning_rate=1e-1, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "global_step = tf.Variable(initial_value=0)\n",
    "train_op    = optimizer.minimize(loss=loss, global_step=global_step, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "The last thing before we can run the whole monster - define train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "# nb_epoch = 150\n",
    "# batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, Y=None):\n",
    "    \"\"\"Takes a part of pandas DF\n",
    "    Returns a pair (X_batch, Y_batch) or only X_batch, where \n",
    "     - X_batch is a dict {key: value} - where a key is name of column and a value is \n",
    "    a matrix which will be passed to corresponding input.\n",
    "     - Y_batch is just a vector with output values\n",
    "    \"\"\"\n",
    "    size = len(X)\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "#         <YOUR CODE HERE>\n",
    "        X_batch = X.iloc[i:i+batch_size]\n",
    "        X_batch = {col: vectorize(X_batch[col].values, token_to_id, 1) for col in text_columns}\n",
    "        \n",
    "        if Y is None:\n",
    "            yield X_batch \n",
    "        else:\n",
    "            Y_batch = Y.values[i:i+batch_size]\n",
    "            yield X_batch, Y_batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input(X_batch,Y_batch=None):\n",
    "    \"\"\"Function takes a batch from iterate_batches and returns a feed_dictionary with placeholders\"\"\"\n",
    "    feed_dct = {placeholders[k]: X_batch[k] for k in text_columns} \n",
    "    if Y_batch is not None:\n",
    "        feed_dct[true_y] = Y_batch\n",
    "    return feed_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    \"\"\"Here you go over X_val and Y_val and count mean errors.\n",
    "    Use iterate_batches and get_input\"\"\"\n",
    "    MSE, AE = 0, 0\n",
    "    batches = 0\n",
    "    for X_batch, Y_batch in iterate_batches(X_val, Y_val):\n",
    "        mse, ae = sess.run([loss,mean_abs_error], get_input(X_batch, Y_batch))\n",
    "        MSE+=mse\n",
    "        AE += ae\n",
    "        batches+=1\n",
    "    MSE/=batches\n",
    "    AE/=batches\n",
    "    return (MSE, AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \"Function inferes prediction by dict of inputs\"\n",
    "    return sess.run(predicted_y, get_input(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object iterate_batches at 0x7f9ec6ee3830>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_batches(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "for first_batch in iterate_batches(X_train, Y_train):\n",
    "    break\n",
    "assert len(first_batch) == 2\n",
    "assert type(first_batch[0]) == dict\n",
    "assert first_batch[1].shape[0]==batch_size\n",
    "assert np.unique([inp.shape[0] for inp in first_batch[0].values()])==batch_size\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "[MSE, AE] = validate()\n",
    "assert MSE < 1e10\n",
    "assert AE < 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batches = len(X_train)/batch_size\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Finally it is time to run training\n",
    "\n",
    "First, shuffle the data\n",
    "\n",
    "\n",
    "By the way, if the trainig processes starts and you achive at least **20k** AE error on validation, you get additional **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(X_train))\n",
    "X_train = X_train.iloc[p]\n",
    "Y_train = Y_train.iloc[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.mkdir('summaries_dir/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.chdir('summaries_dir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.mkdir('train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.mkdir('test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# abs_summary  = tf.summary.scalar('abs_summary', abs_error)\n",
    "# loss_summary = tf.summary.scalar('loss_summary', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Train Summaries\n",
    "# train_summary_op     = tf.summary.merge([loss_summary, abs_summary])\n",
    "# train_summary_dir    = os.path.join('summaries_dir/train/')\n",
    "# train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "# # Dev summaries\n",
    "# test_summary_op      = tf.summary.merge([loss_summary, abs_summary])\n",
    "# test_summary_dir     = os.path.join('summaries_dir/test/')\n",
    "# test_summary_writer  = tf.summary.FileWriter(test_summary_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merged = tf.summary.merge_all()\n",
    "# train_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "# test_writer  = tf.summary.FileWriter(test_summary_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_loss = []\n",
    "cur_abs  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Current step: 1. Current loss is 1.52116e+09. Absolute error is 34418.9\n",
      "Validation. MSE: 1392541952.0, AE: 32506.4736328\n",
      "Current step: 2. Current loss is 1.55796e+09. Absolute error is 34739.7\n",
      "Current step: 3. Current loss is 9.71358e+08. Absolute error is 26264.1\n",
      "Current step: 4. Current loss is 3.03068e+08. Absolute error is 12487.3\n",
      "Current step: 5. Current loss is 3.68544e+09. Absolute error is 58665.8\n",
      "Current step: 6. Current loss is 3.88619e+08. Absolute error is 17369.6\n",
      "Current step: 7. Current loss is 4.93621e+08. Absolute error is 15830.5\n",
      "Current step: 8. Current loss is 9.03917e+08. Absolute error is 25331.3\n",
      "Current step: 9. Current loss is 1.11163e+09. Absolute error is 29045.1\n",
      "Current step: 10. Current loss is 1.17482e+09. Absolute error is 29565.0\n",
      "Current step: 11. Current loss is 1.22855e+09. Absolute error is 30822.9\n",
      "Current step: 12. Current loss is 1.21368e+09. Absolute error is 29928.2\n",
      "Current step: 13. Current loss is 1.19165e+09. Absolute error is 29883.8\n",
      "Current step: 14. Current loss is 1.14122e+09. Absolute error is 28325.0\n",
      "Current step: 15. Current loss is 7.77621e+08. Absolute error is 22499.0\n",
      "Current step: 16. Current loss is 6.50881e+08. Absolute error is 18271.4\n",
      "Current step: 17. Current loss is 3.51235e+08. Absolute error is 13453.6\n",
      "Current step: 18. Current loss is 3.71552e+08. Absolute error is 16491.4\n",
      "Current step: 19. Current loss is 6.96488e+08. Absolute error is 23412.9\n",
      "Current step: 20. Current loss is 5.20948e+08. Absolute error is 20121.2\n",
      "Current step: 21. Current loss is 3.48565e+08. Absolute error is 14878.9\n",
      "Current step: 22. Current loss is 2.74992e+08. Absolute error is 12158.0\n",
      "Current step: 23. Current loss is 3.14786e+08. Absolute error is 11999.5\n",
      "Current step: 24. Current loss is 4.24586e+08. Absolute error is 14640.7\n",
      "Current step: 25. Current loss is 5.09682e+08. Absolute error is 15692.4\n",
      "Current step: 26. Current loss is 4.33749e+08. Absolute error is 14653.7\n",
      "Current step: 27. Current loss is 2.68382e+08. Absolute error is 11653.9\n",
      "Current step: 28. Current loss is 2.33155e+08. Absolute error is 12428.9\n",
      "Current step: 29. Current loss is 3.24157e+08. Absolute error is 15366.4\n",
      "Current step: 30. Current loss is 3.90727e+08. Absolute error is 17395.6\n",
      "Current step: 31. Current loss is 3.00556e+08. Absolute error is 14655.1\n",
      "Validation. MSE: 269707200.0, AE: 12169.8486328\n",
      "Current step: 32. Current loss is 2.27837e+08. Absolute error is 11821.2\n",
      "Current step: 33. Current loss is 2.95192e+08. Absolute error is 11342.3\n",
      "Current step: 34. Current loss is 2.8634e+08. Absolute error is 10941.7\n",
      "Current step: 35. Current loss is 3.17007e+08. Absolute error is 11432.3\n",
      "Current step: 36. Current loss is 3.06679e+08. Absolute error is 11295.5\n",
      "Current step: 37. Current loss is 2.57924e+08. Absolute error is 10840.0\n",
      "Current step: 38. Current loss is 1.75632e+08. Absolute error is 10293.9\n",
      "Current step: 39. Current loss is 2.41638e+08. Absolute error is 12466.4\n",
      "Current step: 40. Current loss is 2.37295e+08. Absolute error is 12561.2\n",
      "Current step: 41. Current loss is 2.15017e+08. Absolute error is 11854.7\n",
      "Current step: 42. Current loss is 2.17674e+08. Absolute error is 10346.7\n",
      "Current step: 43. Current loss is 1.85534e+08. Absolute error is 9578.99\n",
      "Current step: 44. Current loss is 2.39586e+08. Absolute error is 10185.6\n",
      "Current step: 45. Current loss is 1.78357e+08. Absolute error is 9227.56\n",
      "Current step: 46. Current loss is 2.31351e+08. Absolute error is 10196.5\n",
      "Current step: 47. Current loss is 1.76033e+08. Absolute error is 9462.06\n",
      "Current step: 48. Current loss is 1.6048e+08. Absolute error is 9556.54\n",
      "Current step: 49. Current loss is 2.06035e+08. Absolute error is 10621.3\n",
      "Current step: 50. Current loss is 2.01579e+08. Absolute error is 10577.8\n",
      "Current step: 51. Current loss is 1.82047e+08. Absolute error is 9302.14\n",
      "Current step: 52. Current loss is 2.04978e+08. Absolute error is 9847.26\n",
      "Current step: 53. Current loss is 1.72401e+08. Absolute error is 8925.3\n",
      "Current step: 54. Current loss is 1.86867e+08. Absolute error is 9092.18\n",
      "Current step: 55. Current loss is 1.42352e+08. Absolute error is 8372.36\n",
      "Current step: 56. Current loss is 1.41001e+08. Absolute error is 8323.93\n",
      "Current step: 57. Current loss is 1.69011e+08. Absolute error is 9040.21\n",
      "Current step: 58. Current loss is 1.63681e+08. Absolute error is 9352.61\n",
      "Current step: 59. Current loss is 1.56492e+08. Absolute error is 8943.47\n",
      "Current step: 60. Current loss is 1.4738e+08. Absolute error is 8334.64\n",
      "Current step: 61. Current loss is 1.45604e+08. Absolute error is 7981.77\n",
      "Validation. MSE: 179288208.0, AE: 8666.96435547\n",
      "Current step: 62. Current loss is 1.5613e+08. Absolute error is 8635.08\n",
      "Current step: 63. Current loss is 1.41829e+08. Absolute error is 8223.89\n",
      "Current step: 64. Current loss is 1.40842e+08. Absolute error is 8225.19\n",
      "Current step: 65. Current loss is 1.47533e+08. Absolute error is 8309.01\n",
      "Current step: 66. Current loss is 1.41176e+08. Absolute error is 8522.63\n",
      "Current step: 67. Current loss is 1.38458e+08. Absolute error is 8913.02\n",
      "Current step: 68. Current loss is 1.29979e+08. Absolute error is 8318.68\n",
      "Current step: 69. Current loss is 1.56156e+08. Absolute error is 8076.51\n",
      "Current step: 70. Current loss is 1.14798e+08. Absolute error is 7731.25\n",
      "Current step: 71. Current loss is 1.17353e+08. Absolute error is 7511.21\n",
      "Current step: 72. Current loss is 1.47345e+08. Absolute error is 7754.28\n",
      "Current step: 73. Current loss is 1.46536e+08. Absolute error is 8233.75\n",
      "Current step: 74. Current loss is 1.20337e+08. Absolute error is 7641.93\n",
      "Current step: 75. Current loss is 1.72974e+08. Absolute error is 8828.64\n",
      "Current step: 76. Current loss is 1.42692e+08. Absolute error is 8584.08\n",
      "Current step: 77. Current loss is 1.36056e+08. Absolute error is 7996.54\n",
      "Current step: 78. Current loss is 1.24389e+08. Absolute error is 7864.22\n",
      "Current step: 79. Current loss is 2.05949e+08. Absolute error is 8829.77\n",
      "Current step: 80. Current loss is 1.20431e+08. Absolute error is 8006.43\n",
      "Current step: 81. Current loss is 1.29739e+08. Absolute error is 8289.44\n",
      "Current step: 82. Current loss is 1.27309e+08. Absolute error is 8210.7\n",
      "Current step: 83. Current loss is 1.2905e+08. Absolute error is 8064.32\n",
      "Current step: 84. Current loss is 1.24215e+08. Absolute error is 7538.86\n",
      "Current step: 85. Current loss is 1.42897e+08. Absolute error is 8063.59\n",
      "Current step: 86. Current loss is 1.1292e+08. Absolute error is 7472.88\n",
      "Current step: 87. Current loss is 1.47455e+08. Absolute error is 8269.63\n",
      "Current step: 88. Current loss is 1.83827e+08. Absolute error is 9336.29\n",
      "Current step: 89. Current loss is 1.39058e+08. Absolute error is 9050.57\n",
      "Current step: 90. Current loss is 1.26521e+08. Absolute error is 8368.36\n",
      "Current step: 91. Current loss is 1.67598e+08. Absolute error is 8224.67\n",
      "Validation. MSE: 158965856.0, AE: 8002.50488281\n",
      "Current step: 92. Current loss is 1.40707e+08. Absolute error is 7593.38\n",
      "Current step: 93. Current loss is 1.35715e+08. Absolute error is 7954.76\n",
      "Current step: 94. Current loss is 1.32747e+08. Absolute error is 7539.43\n",
      "Current step: 95. Current loss is 1.48432e+08. Absolute error is 8686.97\n",
      "Current step: 96. Current loss is 1.15214e+08. Absolute error is 7843.46\n",
      "Current step: 97. Current loss is 1.0889e+08. Absolute error is 7803.58\n",
      "Current step: 98. Current loss is 1.14916e+08. Absolute error is 7487.4\n",
      "Current step: 99. Current loss is 1.25975e+08. Absolute error is 7324.04\n",
      "Current step: 100. Current loss is 1.36293e+08. Absolute error is 7785.32\n",
      "Current step: 101. Current loss is 1.31099e+08. Absolute error is 8009.54\n",
      "Current step: 102. Current loss is 1.562e+08. Absolute error is 8061.48\n",
      "Current step: 103. Current loss is 1.35749e+08. Absolute error is 8184.77\n",
      "Current step: 104. Current loss is 1.17011e+08. Absolute error is 7920.64\n",
      "Current step: 105. Current loss is 1.13988e+08. Absolute error is 7476.73\n",
      "Current step: 106. Current loss is 1.14317e+08. Absolute error is 7620.98\n",
      "Current step: 107. Current loss is 1.23815e+08. Absolute error is 7626.68\n",
      "Current step: 108. Current loss is 1.1376e+08. Absolute error is 7462.96\n",
      "Current step: 109. Current loss is 1.28032e+08. Absolute error is 7882.94\n",
      "Current step: 110. Current loss is 1.11773e+08. Absolute error is 7392.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 111. Current loss is 1.35848e+08. Absolute error is 8345.68\n",
      "Current step: 112. Current loss is 1.10241e+08. Absolute error is 7596.75\n",
      "Current step: 113. Current loss is 1.1701e+08. Absolute error is 7663.2\n",
      "Current step: 114. Current loss is 1.18416e+08. Absolute error is 7373.56\n",
      "Current step: 115. Current loss is 1.52988e+08. Absolute error is 8228.61\n",
      "Current step: 116. Current loss is 1.21736e+08. Absolute error is 7855.73\n",
      "Current step: 117. Current loss is 1.19209e+08. Absolute error is 7947.72\n",
      "Current step: 118. Current loss is 1.16231e+08. Absolute error is 7752.48\n",
      "Current step: 119. Current loss is 1.16438e+08. Absolute error is 7346.72\n",
      "Current step: 120. Current loss is 1.35356e+08. Absolute error is 7564.21\n",
      "Current step: 121. Current loss is 1.07065e+08. Absolute error is 7192.79\n",
      "Validation. MSE: 141511360.0, AE: 7696.89526367\n",
      "Current step: 122. Current loss is 1.16729e+08. Absolute error is 7413.72\n",
      "Current step: 123. Current loss is 9.34492e+07. Absolute error is 7123.72\n",
      "Current step: 124. Current loss is 1.08426e+08. Absolute error is 7221.3\n",
      "Current step: 125. Current loss is 1.35841e+08. Absolute error is 7992.69\n",
      "Current step: 126. Current loss is 1.59367e+08. Absolute error is 7520.62\n",
      "Current step: 127. Current loss is 1.14768e+08. Absolute error is 7188.15\n",
      "Current step: 128. Current loss is 1.16441e+08. Absolute error is 7666.34\n",
      "Current step: 129. Current loss is 1.20884e+08. Absolute error is 7883.57\n",
      "Current step: 130. Current loss is 1.00422e+08. Absolute error is 7099.8\n",
      "Current step: 131. Current loss is 1.15714e+08. Absolute error is 7561.49\n",
      "Current step: 132. Current loss is 1.01309e+08. Absolute error is 7236.2\n",
      "Current step: 133. Current loss is 1.48025e+08. Absolute error is 8175.13\n",
      "Current step: 134. Current loss is 1.36063e+08. Absolute error is 8112.83\n",
      "Current step: 135. Current loss is 1.4472e+08. Absolute error is 7669.08\n",
      "Current step: 136. Current loss is 1.16925e+08. Absolute error is 7566.74\n",
      "Current step: 137. Current loss is 1.16426e+08. Absolute error is 7530.69\n",
      "Current step: 138. Current loss is 1.26799e+08. Absolute error is 7977.07\n",
      "Current step: 139. Current loss is 1.37469e+08. Absolute error is 7730.93\n",
      "Current step: 140. Current loss is 1.38292e+08. Absolute error is 7884.52\n",
      "Current step: 141. Current loss is 1.12572e+08. Absolute error is 7682.01\n",
      "Current step: 142. Current loss is 1.15739e+08. Absolute error is 7762.21\n",
      "Current step: 143. Current loss is 1.13987e+08. Absolute error is 7166.85\n",
      "Current step: 144. Current loss is 1.31007e+08. Absolute error is 7646.29\n",
      "Current step: 145. Current loss is 9.96466e+07. Absolute error is 6935.92\n",
      "Current step: 146. Current loss is 1.07306e+08. Absolute error is 7246.68\n",
      "Current step: 147. Current loss is 1.13354e+08. Absolute error is 7693.58\n",
      "Current step: 148. Current loss is 1.20708e+08. Absolute error is 8201.26\n",
      "Current step: 149. Current loss is 1.47388e+08. Absolute error is 7407.84\n",
      "Current step: 150. Current loss is 1.22963e+08. Absolute error is 7614.99\n",
      "Current step: 151. Current loss is 1.08119e+08. Absolute error is 7065.84\n",
      "Validation. MSE: 137479520.0, AE: 7678.18164062\n",
      "Current step: 152. Current loss is 9.35258e+07. Absolute error is 6864.08\n",
      "Current step: 153. Current loss is 1.16018e+08. Absolute error is 7199.47\n",
      "Current step: 154. Current loss is 1.11254e+08. Absolute error is 7305.64\n",
      "Current step: 155. Current loss is 1.03892e+08. Absolute error is 7119.66\n",
      "Current step: 156. Current loss is 1.01453e+08. Absolute error is 7232.24\n",
      "Current step: 157. Current loss is 9.59264e+07. Absolute error is 6761.07\n",
      "Current step: 158. Current loss is 1.55706e+08. Absolute error is 7498.37\n",
      "Current step: 159. Current loss is 1.20515e+08. Absolute error is 7474.96\n",
      "Current step: 160. Current loss is 1.48901e+08. Absolute error is 7326.17\n",
      "Current step: 161. Current loss is 1.16799e+08. Absolute error is 7834.29\n",
      "Current step: 162. Current loss is 8.88676e+07. Absolute error is 6942.29\n",
      "Current step: 163. Current loss is 8.83936e+07. Absolute error is 6782.1\n",
      "Current step: 164. Current loss is 1.54149e+08. Absolute error is 8350.11\n",
      "Current step: 165. Current loss is 1.0957e+08. Absolute error is 7428.08\n",
      "Current step: 166. Current loss is 1.11139e+08. Absolute error is 7473.85\n",
      "Current step: 167. Current loss is 1.32983e+08. Absolute error is 8351.93\n",
      "Current step: 168. Current loss is 1.25408e+08. Absolute error is 7633.67\n",
      "Current step: 169. Current loss is 1.20157e+08. Absolute error is 7322.2\n",
      "Current step: 170. Current loss is 1.34427e+08. Absolute error is 7789.71\n",
      "Current step: 171. Current loss is 9.44004e+07. Absolute error is 7306.74\n",
      "Current step: 172. Current loss is 1.09959e+08. Absolute error is 7558.41\n",
      "Current step: 173. Current loss is 9.75864e+07. Absolute error is 7134.9\n",
      "Current step: 174. Current loss is 1.1239e+08. Absolute error is 7440.67\n",
      "Current step: 175. Current loss is 1.1304e+08. Absolute error is 7174.47\n",
      "Current step: 176. Current loss is 1.05688e+08. Absolute error is 7146.68\n",
      "Current step: 177. Current loss is 1.01879e+08. Absolute error is 6992.09\n",
      "Current step: 178. Current loss is 1.77742e+08. Absolute error is 7900.58\n",
      "Current step: 179. Current loss is 9.67565e+07. Absolute error is 7127.98\n",
      "Current step: 180. Current loss is 1.3316e+08. Absolute error is 7712.04\n",
      "Current step: 181. Current loss is 9.56232e+07. Absolute error is 7087.23\n",
      "Validation. MSE: 135697032.0, AE: 7467.92382812\n",
      "Current step: 182. Current loss is 1.12829e+08. Absolute error is 7079.53\n",
      "Current step: 183. Current loss is 1.08309e+08. Absolute error is 7142.39\n",
      "Current step: 184. Current loss is 1.13895e+08. Absolute error is 7605.17\n",
      "Current step: 185. Current loss is 1.0922e+08. Absolute error is 7385.27\n",
      "Current step: 186. Current loss is 9.52882e+07. Absolute error is 7008.88\n",
      "Current step: 187. Current loss is 9.84734e+07. Absolute error is 6927.77\n",
      "Current step: 188. Current loss is 1.07785e+08. Absolute error is 7101.61\n",
      "Current step: 189. Current loss is 1.18916e+08. Absolute error is 7669.37\n",
      "Current step: 190. Current loss is 1.09064e+08. Absolute error is 7348.0\n",
      "Current step: 191. Current loss is 1.00316e+08. Absolute error is 6869.56\n",
      "Current step: 192. Current loss is 1.17341e+08. Absolute error is 6899.45\n",
      "Current step: 193. Current loss is 1.01148e+08. Absolute error is 6981.83\n",
      "Current step: 194. Current loss is 8.70969e+07. Absolute error is 6851.45\n",
      "Current step: 195. Current loss is 1.1386e+08. Absolute error is 7462.48\n",
      "Current step: 196. Current loss is 8.14591e+07. Absolute error is 6842.36\n",
      "Current step: 197. Current loss is 1.51832e+08. Absolute error is 7742.45\n",
      "Current step: 198. Current loss is 1.05562e+08. Absolute error is 6983.86\n",
      "Current step: 199. Current loss is 1.0578e+08. Absolute error is 7308.48\n",
      "Current step: 200. Current loss is 1.13683e+08. Absolute error is 7257.44\n",
      "Current step: 201. Current loss is 1.54365e+08. Absolute error is 8247.27\n",
      "Current step: 202. Current loss is 1.31366e+08. Absolute error is 7407.55\n",
      "Current step: 203. Current loss is 1.09707e+08. Absolute error is 7474.29\n",
      "Current step: 204. Current loss is 9.29428e+07. Absolute error is 6809.83\n",
      "Current step: 205. Current loss is 1.34152e+08. Absolute error is 7467.49\n",
      "Current step: 206. Current loss is 9.60519e+07. Absolute error is 6762.78\n",
      "Current step: 207. Current loss is 1.18422e+08. Absolute error is 7481.31\n",
      "Current step: 208. Current loss is 9.29169e+07. Absolute error is 6808.42\n",
      "Current step: 209. Current loss is 1.13978e+08. Absolute error is 7484.38\n",
      "Current step: 210. Current loss is 1.18132e+08. Absolute error is 7695.47\n",
      "Current step: 211. Current loss is 1.0104e+08. Absolute error is 6936.72\n",
      "Validation. MSE: 128889144.0, AE: 7307.34204102\n",
      "Current step: 212. Current loss is 1.53056e+08. Absolute error is 7477.47\n",
      "Current step: 213. Current loss is 1.51231e+08. Absolute error is 7588.7\n",
      "Current step: 214. Current loss is 1.09577e+08. Absolute error is 7623.45\n",
      "Current step: 215. Current loss is 1.10615e+08. Absolute error is 7105.8\n",
      "Current step: 216. Current loss is 1.24286e+08. Absolute error is 7578.87\n",
      "Current step: 217. Current loss is 1.01669e+08. Absolute error is 7179.18\n",
      "Current step: 218. Current loss is 9.7122e+07. Absolute error is 7067.9\n",
      "Current step: 219. Current loss is 1.14456e+08. Absolute error is 7326.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 220. Current loss is 1.17135e+08. Absolute error is 7480.4\n",
      "Current step: 221. Current loss is 1.10335e+08. Absolute error is 7277.79\n",
      "Current step: 222. Current loss is 1.11219e+08. Absolute error is 7302.62\n",
      "Current step: 223. Current loss is 1.06146e+08. Absolute error is 7120.42\n",
      "Current step: 224. Current loss is 1.15323e+08. Absolute error is 7500.63\n",
      "Current step: 225. Current loss is 1.06145e+08. Absolute error is 7443.88\n",
      "Current step: 226. Current loss is 9.0483e+07. Absolute error is 6777.33\n",
      "Current step: 227. Current loss is 1.41113e+08. Absolute error is 7281.62\n",
      "Current step: 228. Current loss is 1.09033e+08. Absolute error is 7279.67\n",
      "Current step: 229. Current loss is 1.09252e+08. Absolute error is 7280.85\n",
      "Current step: 230. Current loss is 1.12842e+08. Absolute error is 7729.82\n",
      "Current step: 231. Current loss is 1.10812e+08. Absolute error is 7546.75\n",
      "Current step: 232. Current loss is 1.12402e+08. Absolute error is 7188.42\n",
      "Current step: 233. Current loss is 1.43484e+08. Absolute error is 7682.77\n",
      "Current step: 234. Current loss is 1.12535e+08. Absolute error is 7013.81\n",
      "Current step: 235. Current loss is 1.35242e+08. Absolute error is 7711.16\n",
      "Current step: 236. Current loss is 1.07205e+08. Absolute error is 7590.12\n",
      "Current step: 237. Current loss is 1.28486e+08. Absolute error is 7506.97\n",
      "Current step: 238. Current loss is 9.29602e+07. Absolute error is 6676.94\n",
      "Current step: 239. Current loss is 8.89756e+07. Absolute error is 6880.77\n",
      "Current step: 240. Current loss is 1.08958e+08. Absolute error is 7021.41\n",
      "Current step: 241. Current loss is 1.21544e+08. Absolute error is 7764.36\n",
      "Validation. MSE: 129823128.0, AE: 7623.2980957\n",
      "Current step: 242. Current loss is 1.03295e+08. Absolute error is 6755.94\n",
      "Current step: 243. Current loss is 1.15965e+08. Absolute error is 7393.92\n",
      "Current step: 244. Current loss is 8.95496e+07. Absolute error is 6679.18\n",
      "Current step: 245. Current loss is 1.94617e+08. Absolute error is 7930.43\n",
      "Current step: 246. Current loss is 1.18794e+08. Absolute error is 7882.8\n",
      "Current step: 247. Current loss is 8.9139e+07. Absolute error is 6969.26\n",
      "Current step: 248. Current loss is 8.64526e+07. Absolute error is 6553.21\n",
      "Current step: 249. Current loss is 1.05056e+08. Absolute error is 7101.38\n",
      "Current step: 250. Current loss is 9.95297e+07. Absolute error is 6752.58\n",
      "Current step: 251. Current loss is 1.05847e+08. Absolute error is 7052.39\n",
      "Current step: 252. Current loss is 1.50787e+08. Absolute error is 7949.52\n",
      "Current step: 253. Current loss is 9.29749e+07. Absolute error is 7018.5\n",
      "Current step: 254. Current loss is 1.13181e+08. Absolute error is 7504.17\n",
      "Current step: 255. Current loss is 9.15728e+07. Absolute error is 6662.19\n",
      "Current step: 256. Current loss is 9.90065e+07. Absolute error is 6906.24\n",
      "Current step: 257. Current loss is 9.46271e+07. Absolute error is 6773.47\n",
      "Current step: 258. Current loss is 1.12292e+08. Absolute error is 7460.15\n",
      "Current step: 259. Current loss is 1.38717e+08. Absolute error is 8105.01\n",
      "Current step: 260. Current loss is 1.10689e+08. Absolute error is 7215.2\n",
      "Current step: 261. Current loss is 1.16469e+08. Absolute error is 7360.01\n",
      "Current step: 262. Current loss is 1.50958e+08. Absolute error is 7217.12\n",
      "Current step: 263. Current loss is 1.06045e+08. Absolute error is 7305.7\n",
      "Current step: 264. Current loss is 9.64522e+07. Absolute error is 7193.42\n",
      "Current step: 265. Current loss is 1.03549e+08. Absolute error is 7390.05\n",
      "Current step: 266. Current loss is 9.42913e+07. Absolute error is 6639.94\n",
      "Current step: 267. Current loss is 9.71521e+07. Absolute error is 6903.15\n",
      "Current step: 268. Current loss is 1.31325e+08. Absolute error is 7394.18\n",
      "Current step: 269. Current loss is 1.01417e+08. Absolute error is 7103.12\n",
      "Current step: 270. Current loss is 1.05708e+08. Absolute error is 6939.65\n",
      "Current step: 271. Current loss is 1.20008e+08. Absolute error is 7572.98\n",
      "Validation. MSE: 125419680.0, AE: 7246.71362305\n",
      "Current step: 272. Current loss is 1.47332e+08. Absolute error is 7216.14\n",
      "Current step: 273. Current loss is 1.44559e+08. Absolute error is 7630.42\n",
      "Current step: 274. Current loss is 9.27857e+07. Absolute error is 6662.52\n",
      "Current step: 275. Current loss is 1.06931e+08. Absolute error is 7367.1\n",
      "Current step: 276. Current loss is 8.20088e+07. Absolute error is 6415.74\n",
      "Current step: 277. Current loss is 9.50389e+07. Absolute error is 6600.01\n",
      "Current step: 278. Current loss is 1.1087e+08. Absolute error is 7263.18\n",
      "Current step: 279. Current loss is 1.03001e+08. Absolute error is 6935.42\n",
      "Current step: 280. Current loss is 9.75587e+07. Absolute error is 6919.99\n",
      "Current step: 281. Current loss is 1.35487e+08. Absolute error is 7664.87\n",
      "Current step: 282. Current loss is 1.16977e+08. Absolute error is 7487.07\n",
      "Current step: 283. Current loss is 1.11127e+08. Absolute error is 7190.88\n",
      "Current step: 284. Current loss is 8.65057e+07. Absolute error is 6553.01\n",
      "Current step: 285. Current loss is 1.13358e+08. Absolute error is 7492.05\n",
      "Current step: 286. Current loss is 9.6636e+07. Absolute error is 7148.75\n",
      "Current step: 287. Current loss is 1.30306e+08. Absolute error is 8150.03\n",
      "Current step: 288. Current loss is 1.0283e+08. Absolute error is 7271.08\n",
      "Current step: 289. Current loss is 1.12709e+08. Absolute error is 7155.07\n",
      "Current step: 290. Current loss is 1.13793e+08. Absolute error is 7402.26\n",
      "Current step: 291. Current loss is 1.20034e+08. Absolute error is 8002.81\n",
      "Current step: 292. Current loss is 1.01831e+08. Absolute error is 6906.36\n",
      "Current step: 293. Current loss is 9.93036e+07. Absolute error is 6917.91\n",
      "Current step: 294. Current loss is 1.02294e+08. Absolute error is 6903.98\n",
      "Current step: 295. Current loss is 9.40798e+07. Absolute error is 6981.9\n",
      "Current step: 296. Current loss is 1.13678e+08. Absolute error is 7816.19\n",
      "Current step: 297. Current loss is 1.27934e+08. Absolute error is 7800.29\n",
      "Current step: 298. Current loss is 1.09965e+08. Absolute error is 7164.46\n",
      "Current step: 299. Current loss is 1.08495e+08. Absolute error is 7181.18\n",
      "Current step: 300. Current loss is 1.09281e+08. Absolute error is 7149.01\n",
      "Current step: 301. Current loss is 9.5227e+07. Absolute error is 6793.04\n",
      "Validation. MSE: 129368056.0, AE: 7580.56689453\n",
      "Current step: 302. Current loss is 1.23676e+08. Absolute error is 7396.46\n",
      "Current step: 303. Current loss is 8.6618e+07. Absolute error is 6746.89\n",
      "Current step: 304. Current loss is 1.13375e+08. Absolute error is 6951.29\n",
      "Current step: 305. Current loss is 9.46514e+07. Absolute error is 7134.73\n",
      "Current step: 306. Current loss is 1.10724e+08. Absolute error is 7039.92\n",
      "Current step: 307. Current loss is 8.91886e+07. Absolute error is 6582.69\n",
      "Current step: 308. Current loss is 1.15754e+08. Absolute error is 7316.79\n",
      "Current step: 309. Current loss is 1.18876e+08. Absolute error is 7375.34\n",
      "Current step: 310. Current loss is 9.38231e+07. Absolute error is 6706.95\n",
      "Current step: 311. Current loss is 1.10913e+08. Absolute error is 7012.33\n",
      "Current step: 312. Current loss is 1.03786e+08. Absolute error is 7496.78\n",
      "Current step: 313. Current loss is 1.09995e+08. Absolute error is 7429.1\n",
      "Current step: 314. Current loss is 1.15527e+08. Absolute error is 6938.55\n",
      "Current step: 315. Current loss is 1.05287e+08. Absolute error is 6753.65\n",
      "Current step: 316. Current loss is 1.21831e+08. Absolute error is 6963.93\n",
      "Current step: 317. Current loss is 9.64748e+07. Absolute error is 6962.14\n",
      "Current step: 318. Current loss is 1.03686e+08. Absolute error is 7324.98\n",
      "Current step: 319. Current loss is 1.32108e+08. Absolute error is 7647.8\n",
      "Current step: 320. Current loss is 1.03003e+08. Absolute error is 6995.78\n",
      "Current step: 321. Current loss is 1.60663e+08. Absolute error is 7860.3\n",
      "Current step: 322. Current loss is 1.0526e+08. Absolute error is 7232.22\n",
      "Current step: 323. Current loss is 1.6149e+08. Absolute error is 7858.47\n",
      "Current step: 324. Current loss is 9.25063e+07. Absolute error is 6660.33\n",
      "Current step: 325. Current loss is 1.07066e+08. Absolute error is 7205.4\n",
      "Current step: 326. Current loss is 1.11132e+08. Absolute error is 7312.69\n",
      "Current step: 327. Current loss is 9.03317e+07. Absolute error is 6767.95\n",
      "Current step: 328. Current loss is 8.76114e+07. Absolute error is 6834.31\n",
      "Current step: 329. Current loss is 1.47674e+08. Absolute error is 7859.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 330. Current loss is 1.28707e+08. Absolute error is 7686.39\n",
      "Current step: 331. Current loss is 1.25087e+08. Absolute error is 7483.92\n",
      "Validation. MSE: 125226808.0, AE: 7179.33178711\n",
      "Current step: 332. Current loss is 1.5666e+08. Absolute error is 7459.97\n",
      "Current step: 333. Current loss is 1.5944e+08. Absolute error is 8221.49\n",
      "Current step: 334. Current loss is 1.53249e+08. Absolute error is 7347.29\n",
      "Current step: 335. Current loss is 1.42994e+08. Absolute error is 8622.03\n",
      "Current step: 336. Current loss is 1.13196e+08. Absolute error is 7998.35\n",
      "Current step: 337. Current loss is 1.39059e+08. Absolute error is 7327.19\n",
      "Current step: 338. Current loss is 1.14928e+08. Absolute error is 7504.83\n",
      "Current step: 339. Current loss is 1.29488e+08. Absolute error is 7765.7\n",
      "Current step: 340. Current loss is 1.16866e+08. Absolute error is 8152.06\n",
      "Current step: 341. Current loss is 8.13575e+07. Absolute error is 6681.03\n",
      "Current step: 342. Current loss is 1.09099e+08. Absolute error is 7332.83\n",
      "Current step: 343. Current loss is 1.06064e+08. Absolute error is 7120.4\n",
      "Current step: 344. Current loss is 1.00703e+08. Absolute error is 6958.27\n",
      "Current step: 345. Current loss is 9.82585e+07. Absolute error is 7401.5\n",
      "Current step: 346. Current loss is 1.05034e+08. Absolute error is 6999.85\n",
      "Current step: 347. Current loss is 8.69126e+07. Absolute error is 6442.78\n",
      "Current step: 348. Current loss is 9.78171e+07. Absolute error is 6895.78\n",
      "Current step: 349. Current loss is 1.02554e+08. Absolute error is 7075.28\n",
      "Current step: 350. Current loss is 8.57253e+07. Absolute error is 6831.1\n",
      "Current step: 351. Current loss is 1.10601e+08. Absolute error is 6954.76\n",
      "Current step: 352. Current loss is 7.2341e+07. Absolute error is 6126.83\n",
      "Current step: 353. Current loss is 1.02502e+08. Absolute error is 7078.63\n",
      "Current step: 354. Current loss is 8.71632e+07. Absolute error is 6646.31\n",
      "Current step: 355. Current loss is 7.69293e+07. Absolute error is 6295.16\n",
      "Current step: 356. Current loss is 9.55171e+07. Absolute error is 6494.66\n",
      "Current step: 357. Current loss is 7.4729e+07. Absolute error is 6237.31\n",
      "Current step: 358. Current loss is 8.57488e+07. Absolute error is 6754.19\n",
      "Current step: 359. Current loss is 8.34944e+07. Absolute error is 6536.45\n",
      "Current step: 360. Current loss is 8.74576e+07. Absolute error is 6464.35\n",
      "Current step: 361. Current loss is 1.24755e+08. Absolute error is 7209.28\n",
      "Validation. MSE: 124602644.0, AE: 7377.05761719\n",
      "Current step: 362. Current loss is 1.11402e+08. Absolute error is 7559.16\n",
      "Current step: 363. Current loss is 1.04826e+08. Absolute error is 7415.42\n",
      "Current step: 364. Current loss is 1.0456e+08. Absolute error is 6699.84\n",
      "Current step: 365. Current loss is 1.15188e+08. Absolute error is 7479.58\n",
      "Current step: 366. Current loss is 8.94528e+07. Absolute error is 6675.35\n",
      "Current step: 367. Current loss is 8.58401e+07. Absolute error is 6933.71\n",
      "Current step: 368. Current loss is 1.00595e+08. Absolute error is 7022.33\n",
      "Current step: 369. Current loss is 9.45732e+07. Absolute error is 6657.15\n",
      "Current step: 370. Current loss is 9.58141e+07. Absolute error is 6457.34\n",
      "Current step: 371. Current loss is 9.97521e+07. Absolute error is 6975.8\n",
      "Current step: 372. Current loss is 1.12882e+08. Absolute error is 7146.54\n",
      "Current step: 373. Current loss is 1.54393e+08. Absolute error is 7505.51\n",
      "Current step: 374. Current loss is 1.04883e+08. Absolute error is 7282.69\n",
      "Current step: 375. Current loss is 1.11957e+08. Absolute error is 7636.92\n",
      "Current step: 376. Current loss is 1.01894e+08. Absolute error is 7116.57\n",
      "Current step: 377. Current loss is 1.05769e+08. Absolute error is 7208.62\n",
      "Current step: 378. Current loss is 1.00514e+08. Absolute error is 7130.73\n",
      "Current step: 379. Current loss is 1.12892e+08. Absolute error is 7348.67\n",
      "Current step: 380. Current loss is 8.41698e+07. Absolute error is 6174.88\n",
      "Current step: 381. Current loss is 1.10042e+08. Absolute error is 7342.7\n",
      "Current step: 382. Current loss is 9.51861e+07. Absolute error is 7058.78\n",
      "Current step: 383. Current loss is 9.85919e+07. Absolute error is 6797.41\n",
      "Current step: 384. Current loss is 1.0226e+08. Absolute error is 6776.65\n",
      "Current step: 385. Current loss is 8.66617e+07. Absolute error is 6653.55\n",
      "Current step: 386. Current loss is 1.00851e+08. Absolute error is 7463.59\n",
      "Current step: 387. Current loss is 1.20313e+08. Absolute error is 7697.35\n",
      "Current step: 388. Current loss is 1.00337e+08. Absolute error is 6818.31\n",
      "Current step: 389. Current loss is 9.02478e+07. Absolute error is 6558.23\n",
      "Current step: 390. Current loss is 1.00264e+08. Absolute error is 6911.46\n",
      "Current step: 391. Current loss is 8.49883e+07. Absolute error is 6708.57\n",
      "Validation. MSE: 122633496.0, AE: 7408.77832031\n",
      "Current step: 392. Current loss is 8.13044e+07. Absolute error is 6149.4\n",
      "Current step: 393. Current loss is 9.02617e+07. Absolute error is 6677.77\n",
      "Current step: 394. Current loss is 1.05077e+08. Absolute error is 6834.76\n",
      "Current step: 395. Current loss is 1.14646e+08. Absolute error is 7138.38\n",
      "Current step: 396. Current loss is 9.24679e+07. Absolute error is 7047.84\n",
      "Current step: 397. Current loss is 9.1298e+07. Absolute error is 6979.52\n",
      "Current step: 398. Current loss is 1.16497e+08. Absolute error is 7020.7\n",
      "Current step: 399. Current loss is 1.11759e+08. Absolute error is 6990.54\n",
      "Current step: 400. Current loss is 1.10233e+08. Absolute error is 7044.0\n",
      "Current step: 401. Current loss is 9.08836e+07. Absolute error is 6938.07\n",
      "Current step: 402. Current loss is 9.36433e+07. Absolute error is 6608.98\n",
      "Current step: 403. Current loss is 1.00142e+08. Absolute error is 6786.45\n",
      "Current step: 404. Current loss is 1.30548e+08. Absolute error is 7428.41\n",
      "Current step: 405. Current loss is 1.00663e+08. Absolute error is 7165.93\n",
      "Current step: 406. Current loss is 1.19259e+08. Absolute error is 7391.12\n",
      "Current step: 407. Current loss is 1.00029e+08. Absolute error is 6736.92\n",
      "Current step: 408. Current loss is 1.41939e+08. Absolute error is 7469.31\n",
      "Current step: 409. Current loss is 8.9375e+07. Absolute error is 6540.11\n",
      "Current step: 410. Current loss is 8.72186e+07. Absolute error is 6818.71\n",
      "Current step: 411. Current loss is 1.06026e+08. Absolute error is 7435.61\n",
      "Current step: 412. Current loss is 9.35072e+07. Absolute error is 7013.16\n",
      "Current step: 413. Current loss is 8.27558e+07. Absolute error is 6494.89\n",
      "Current step: 414. Current loss is 8.90553e+07. Absolute error is 6609.65\n",
      "Current step: 415. Current loss is 8.85089e+07. Absolute error is 6791.22\n",
      "Current step: 416. Current loss is 1.05393e+08. Absolute error is 7322.67\n",
      "Current step: 417. Current loss is 1.03771e+08. Absolute error is 7248.51\n",
      "Current step: 418. Current loss is 1.6063e+08. Absolute error is 7414.93\n",
      "Current step: 419. Current loss is 9.1749e+07. Absolute error is 6726.68\n",
      "Current step: 420. Current loss is 1.11627e+08. Absolute error is 7663.49\n",
      "Current step: 421. Current loss is 1.04498e+08. Absolute error is 7201.17\n",
      "Validation. MSE: 119114260.0, AE: 7054.70410156\n",
      "Current step: 422. Current loss is 1.20802e+08. Absolute error is 7208.64\n",
      "Current step: 423. Current loss is 8.77152e+07. Absolute error is 6874.01\n",
      "Current step: 424. Current loss is 1.02699e+08. Absolute error is 6966.99\n",
      "Current step: 425. Current loss is 9.72234e+07. Absolute error is 6872.29\n",
      "Current step: 426. Current loss is 8.3516e+07. Absolute error is 6448.11\n",
      "Current step: 427. Current loss is 9.70018e+07. Absolute error is 6960.58\n",
      "Current step: 428. Current loss is 8.93761e+07. Absolute error is 6475.55\n",
      "Current step: 429. Current loss is 1.20328e+08. Absolute error is 7675.38\n",
      "Current step: 430. Current loss is 9.44128e+07. Absolute error is 6803.81\n",
      "Current step: 431. Current loss is 9.36567e+07. Absolute error is 6451.0\n",
      "Current step: 432. Current loss is 8.64413e+07. Absolute error is 6713.33\n",
      "Current step: 433. Current loss is 9.13534e+07. Absolute error is 6764.93\n",
      "Current step: 434. Current loss is 1.27637e+08. Absolute error is 7439.43\n",
      "Current step: 435. Current loss is 1.09942e+08. Absolute error is 7297.33\n",
      "Current step: 436. Current loss is 8.62901e+07. Absolute error is 6768.18\n",
      "Current step: 437. Current loss is 9.97206e+07. Absolute error is 6993.6\n",
      "Current step: 438. Current loss is 9.61118e+07. Absolute error is 6736.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 439. Current loss is 9.63515e+07. Absolute error is 7160.17\n",
      "Current step: 440. Current loss is 9.40605e+07. Absolute error is 7109.53\n",
      "Current step: 441. Current loss is 8.61694e+07. Absolute error is 6718.89\n",
      "Current step: 442. Current loss is 1.07407e+08. Absolute error is 7176.87\n",
      "Current step: 443. Current loss is 1.12876e+08. Absolute error is 7329.6\n",
      "Current step: 444. Current loss is 8.08224e+07. Absolute error is 6572.39\n",
      "Current step: 445. Current loss is 1.04772e+08. Absolute error is 7346.22\n",
      "Current step: 446. Current loss is 1.01501e+08. Absolute error is 6873.51\n",
      "Current step: 447. Current loss is 9.75625e+07. Absolute error is 6655.59\n",
      "Current step: 448. Current loss is 9.01684e+07. Absolute error is 6863.91\n",
      "Current step: 449. Current loss is 8.60784e+07. Absolute error is 6885.18\n",
      "Current step: 450. Current loss is 7.62242e+07. Absolute error is 6304.97\n",
      "Current step: 451. Current loss is 1.00541e+08. Absolute error is 6962.66\n",
      "Validation. MSE: 123171724.0, AE: 7069.08837891\n",
      "Current step: 452. Current loss is 9.25293e+07. Absolute error is 6829.56\n",
      "Current step: 453. Current loss is 9.75382e+07. Absolute error is 6689.47\n",
      "Current step: 454. Current loss is 1.04292e+08. Absolute error is 7145.62\n",
      "Current step: 455. Current loss is 1.06773e+08. Absolute error is 6955.04\n",
      "Current step: 456. Current loss is 1.15034e+08. Absolute error is 7254.84\n",
      "Current step: 457. Current loss is 1.19029e+08. Absolute error is 7257.02\n",
      "Current step: 458. Current loss is 8.93219e+07. Absolute error is 6506.42\n",
      "Current step: 459. Current loss is 1.00969e+08. Absolute error is 7000.22\n",
      "Current step: 460. Current loss is 1.35225e+08. Absolute error is 7815.62\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-2de6ad31baaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_abs_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcur_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-caaa30a8954a>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         <YOUR CODE HERE>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_columns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-caaa30a8954a>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         <YOUR CODE HERE>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_columns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-814550254da1>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(strings, token_to_id, UNK)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtoken_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e6b2a924c25f>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "for j in range(num_epochs):\n",
    "    print(j)\n",
    "    for i, (X_batch, Y_batch) in  enumerate(iterate_batches(X_train, Y_train)):\n",
    "        _, step, current_loss, abs_error = sess.run([train_op, global_step, loss, mean_abs_error], get_input(X_batch,Y_batch))\n",
    "        cur_loss.append(current_loss)\n",
    "        cur_abs.append(abs_error)\n",
    "#         train_summary_writer.add_summary(summaries, step)\n",
    "        print(\"Current step: %s. Current loss is %s. Absolute error is %s\" % (step, current_loss, abs_error))\n",
    "        if i % 30 == 0:\n",
    "            print(\"Validation. MSE: %s, AE: %s\" % validate())\n",
    "#         if np.abs(cur_abs[i-1] - cur_abs[i]) >= 1000:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAHwCAYAAAAIDnN0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAOSMQAYAAJCz/w9J6LdiGjFmGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e72fb30b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x = np.linspace(0, np.size(cur_loss))\n",
    "plt.figure(figsize=[10, 8])\n",
    "plt.title(\"Mean squared loss vs step number\")\n",
    "plt.plot(cur_loss, 'red', linewidth=0.5)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MAE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAHwCAYAAAAvoPKcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBs6ETpIkaeBM6CRJkgbOhE6SJGng/gao4dHBEJbG2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e5823e780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10, 8])\n",
    "plt.title(\"Absolute loss vs step number\")\n",
    "plt.plot(cur_abs, 'blue', linewidth=0.5)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MAE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=summaries_dir/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I earned 3 pts with 7001.95556641 absolute error!\n"
     ]
    }
   ],
   "source": [
    "MSE, AE = validate()\n",
    "assert AE < 20000\n",
    "print (\"I earned 3 pts with %s absolute error!\" % AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Done\n",
    "     + **(1 pt)** Add visualisation of train and val loss. Try to use some smoothing to make plots more readable \n",
    "     + **(2 pt)** Try different CNN architectures. Vary kernel size, number of filters. Find out if there is some change to a training process.\n",
    "     + Try to use different architectures for different inputs. Maybe a smaller architecture would be fine for description field and more complex for a title.\n",
    "     + Find out the best **embedding size** value \n",
    "     + Add dropout, \n",
    "     - and some regularisation \n",
    " \n",
    " **(2++ pts) for all experimenting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, according to experiments, there were established some values of variables that I cant explain easily.\n",
    "\n",
    "Firstly, I forked input and did convolutions and poolings, then merged and dropouted 50 percent. Works nice. \n",
    "\n",
    "Secondly, I tried to use some different stuff such average for short texts as title, but it was not any better in terms of stability of learning. \n",
    "\n",
    "Thirdly, I changed batch size, because it took ages to calculate anything. If I had stronge gpu so I cant did 500. Yet its only 300\n",
    "\n",
    "Fourthly, as for embedding size, It was found out empirically that the best one is 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See in the next series\n",
    " - Recurrent neural networks\n",
    "  - Why everybody like them\n",
    "  - Why everybody hate them\n",
    " - Attention for text processing\n",
    "  - How to boost your NLP model performance by implementing recent DL paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
